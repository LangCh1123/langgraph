{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c4bd28",
   "metadata": {},
   "source": [
    "# How to stream arbitrary nested content\n",
    "\n",
    "The most common use case for streaming from inside a node is to stream LLM tokens, but you may have other long-running streaming functions you wish to render for the user. While individual nodes in LangGraph cannot return generators (since they are executed to completion for each [superstep](https://langchain-ai.github.io/langgraph/concepts/low_level)), we can still stream content from arbitrary custom functions from within a node.\n",
    "\n",
    "We do so in two ways:\n",
    "* using graph's `.stream` / `.astream` methods with `stream_mode=\"custom\"`\n",
    "* using [RunnableLambda](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableLambda.html#langchain_core.runnables.base.RunnableLambda) and emitting custom events using [adispatch_custom_events](https://python.langchain.com/docs/how_to/callbacks_custom_events/).\n",
    "\n",
    "Below is a simple toy example that shows both.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install our required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a20f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12297071",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29814253-ca9b-4844-a8a5-d6b19fbdbdba",
   "metadata": {},
   "source": [
    "## Stream nested content using `.stream / .astream`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729644a-b65f-4e69-ad45-f2e88ffb4e9d",
   "metadata": {},
   "source": [
    "### Define the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9731c40f-5ce7-460d-b2ad-33185529c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.graph import START, StateGraph, MessagesState, END\n",
    "from langgraph.types import StreamWriter\n",
    "\n",
    "async def my_generator(state: MessagesState):\n",
    "    messages = [\n",
    "        \"Four\",\n",
    "        \"score\",\n",
    "        \"and\",\n",
    "        \"seven\",\n",
    "        \"years\",\n",
    "        \"ago\",\n",
    "        \"our\",\n",
    "        \"fathers\",\n",
    "        \"...\",\n",
    "    ]\n",
    "    for message in messages:\n",
    "        yield message\n",
    "\n",
    "\n",
    "async def my_node(\n",
    "    state: MessagesState, \n",
    "    writer: StreamWriter  # <-- provide StreamWriter to write chunks to be streamed\n",
    "):\n",
    "    messages = []\n",
    "    async for message in my_generator(state):\n",
    "        # write the chunk to be streamed using stream_mode=custom \n",
    "        writer(message)\n",
    "        messages.append(message)\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=\" \".join(messages))]}\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"model\", my_node)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_edge(\"model\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd69eed-9624-4640-b0af-c9f82b190900",
   "metadata": {},
   "source": [
    "### Stream content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a91b15-82c7-443c-acb6-a7406df15cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four|score|and|seven|years|ago|our|fathers|...|"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = [HumanMessage(content=\"What are you thinking about?\")]\n",
    "async for chunk in app.astream({\"messages\": inputs}, stream_mode=\"custom\"):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29035302-3111-45bf-ac69-50ab940f8cb4",
   "metadata": {},
   "source": [
    "## Stream nested content using `.astream_events`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e91c3-03be-4778-9fa5-a6ec57be3e52",
   "metadata": {},
   "source": [
    "If you are already using graph's `.astream_events` method in your workflow, you can also stream arbitrary nested content using `RunnableLambda` and `adispatch_custom_event`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb6c3e5-7377-4f93-a8c6-44582ee3bc1a",
   "metadata": {},
   "source": [
    "<div class=\"admonition warning\">\n",
    "    <p class=\"admonition-title\">ASYNC IN PYTHON<=3.10</p>\n",
    "    <p>\n",
    "\n",
    "LangChain cannot automatically propagate configuration, including callbacks necessary for `astream_events()`, to child runnables if you are running async code in python<=3.10. This is a common reason why you may fail to see events being emitted from custom runnables or tools.\n",
    "\n",
    "If you are running python<=3.10, you will need to manually propagate the `RunnableConfig` object to the child runnable in async environments. For an example of how to manually propagate the config, see the implementation of the `RunnableLambda` below.\n",
    "\n",
    "If you are running python>=3.11, the RunnableConfig will automatically propagate to child runnables in async environment. However, it is still a good idea to propagate the `RunnableConfig` manually if your code may run in other Python versions.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390a9fe-2d5f-4e82-a1ea-c7c0186b8559",
   "metadata": {},
   "source": [
    "### Define the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486a01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.callbacks.manager import adispatch_custom_event\n",
    "\n",
    "@RunnableLambda\n",
    "async def my_generator(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig  # <-- propagate config\n",
    "):\n",
    "    messages = [\n",
    "        \"Four\",\n",
    "        \"score\",\n",
    "        \"and\",\n",
    "        \"seven\",\n",
    "        \"years\",\n",
    "        \"ago\",\n",
    "        \"our\",\n",
    "        \"fathers\",\n",
    "        \"...\",\n",
    "    ]\n",
    "    for message in messages:\n",
    "        await adispatch_custom_event(\n",
    "            \"my_generator\",\n",
    "            {\"chunk\": message},\n",
    "            config=config  # <-- propagate config\n",
    "        )\n",
    "        yield message\n",
    "\n",
    "\n",
    "async def my_node(state: MessagesState, config: RunnableConfig):\n",
    "    messages = []\n",
    "    async for message in my_generator.astream(state, config):\n",
    "        messages.append(message)\n",
    "    return {\"messages\": [AIMessage(content=\" \".join(messages))]}\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"model\", my_node)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_edge(\"model\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcded03-6776-405e-afae-005a3212d3e4",
   "metadata": {},
   "source": [
    "### Stream content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce773a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four|score|and|seven|years|ago|our|fathers|...|"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = [HumanMessage(content=\"What are you thinking about?\")]\n",
    "async for event in app.astream_events({\"messages\": inputs}, version=\"v2\"):\n",
    "    tags = event.get(\"tags\", [])\n",
    "    if event[\"event\"] == \"on_custom_event\" and event[\"name\"] == \"my_generator\":\n",
    "        data = event[\"data\"]\n",
    "        if data:\n",
    "            print(data[\"chunk\"], end=\"|\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
