{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2eecb96-cf0e-47ed-8116-88a7eaa4236d",
   "metadata": {},
   "source": [
    "# How to share state between threads\n",
    "\n",
    "By default, LangGraph state is scoped to a single thread. LangGraph also allows you to store information that can be **shared** across threads.\n",
    "\n",
    "For instance, you can persist each user’s preferences to a shared memory and reuse them in new conversational threads.\n",
    "\n",
    "In this guide, we will show how to construct and use a graph that has a shared memory implemented using the `Store` interface.\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "    Support for the <code>Store</code> API that is used in this notebook was added in LangGraph <code>v0.2.29</code>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set our API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3457aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa2c64a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6817d",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c550b5-1954-496b-8b9d-800361af17dc",
   "metadata": {},
   "source": [
    "## Create graph\n",
    "\n",
    "In this example we will create a graph that will let us store information about a user's preferences. We will do so by defining an `InMemoryStore` - an object that can store data in memory and query that data. We can then pass the store object when compiling the graph. This allows each node in the graph to access the store: when you define node functions, you can define `store` keyword argument, and LangGraph will automatically pass the store object you compiled the graph with.\n",
    "\n",
    "When storing objects using the `Store` interface you define two things:\n",
    "\n",
    "* the namespace for the object, a tuple (similar to directories)\n",
    "* the object key (similar to filenames)\n",
    "\n",
    "In our example, we'll be using `(\"memories\", <user_id>)` as namespace and random UUID as key for each new memory.\n",
    "\n",
    "Importantly, to determine the user, we will be passing `user_id` via the config keyword argument of each node function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f303d6-612e-4e34-bf36-29d4ed25d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, TypedDict, Annotated\n",
    "import uuid\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langgraph.graph.state import StateGraph, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "# We will give this as a tool to the agent\n",
    "# This will let the agent call this tool to save a fact\n",
    "class Info(TypedDict):\n",
    "    \"\"\"This tool should be called when you want to save a new fact about the user.\n",
    "\n",
    "    Attributes:\n",
    "        fact (str): A fact about the user.\n",
    "        topic (str): The topic related the fact is about, i.e. Food, Location, Movies, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    fact: str\n",
    "    topic: str\n",
    "\n",
    "\n",
    "# This is the prompt we give the agent\n",
    "# We will pass known info into the prompt\n",
    "# We will tell it to use the Info tool to save more\n",
    "prompt = \"\"\"You are a helpful assistant that learns about users to provide better assistance.\n",
    "\n",
    "Current user information:\n",
    "<info>\n",
    "{info}\n",
    "</info>\n",
    "\n",
    "Instructions:\n",
    "1. Use the `Info` tool to save new information the user shares.\n",
    "2. Save facts, opinions, preferences, and experiences.\n",
    "3. Your goal: Improve assistance by building a user profile over time.\n",
    "\n",
    "Remember: Every piece of information helps you serve the user better in future interactions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# We give the model access to the Info tool\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools([Info])\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Call the model.\"\"\"\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    memories = store.search((\"memories\", user_id))\n",
    "    info = \"\\n\".join([d.value[\"fact\"] for d in memories])\n",
    "    # Format system prompt\n",
    "    system_msg = prompt.format(info=info)\n",
    "    # Call model\n",
    "    response = model.invoke(\n",
    "        [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "    )\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Routing function to decide what to do next\n",
    "# If no tool calls, then we end\n",
    "# If tool calls, then we update memory\n",
    "def route(state):\n",
    "    if len(state[\"messages\"][-1].tool_calls) == 0:\n",
    "        return END\n",
    "    else:\n",
    "        return \"update_memory\"\n",
    "\n",
    "\n",
    "def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Update the memory.\"\"\"\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    memory_id = str(uuid.uuid4())\n",
    "    tool_calls = []\n",
    "    memories = {}\n",
    "    # Each tool call is a new memory to save\n",
    "    for tc in state[\"messages\"][-1].tool_calls:\n",
    "        # We append ToolMessages (to pass back to the LLM)\n",
    "        # This is needed because OpenAI requires each tool call be followed by a ToolMessage\n",
    "        tool_calls.append(\n",
    "            {\"role\": \"tool\", \"content\": \"Saved!\", \"tool_call_id\": tc[\"id\"]}\n",
    "        )\n",
    "        # We create a new memory from this tool call\n",
    "        store.put((\"memories\", user_id), memory_id, {\n",
    "            \"fact\": tc[\"args\"][\"fact\"],\n",
    "            \"topic\": tc[\"args\"][\"topic\"],\n",
    "        })\n",
    "    # Return the messages and memories to update the state with\n",
    "    return {\"messages\": tool_calls}\n",
    "\n",
    "\n",
    "# This is the in memory checkpointer we will use\n",
    "# We need this because we want to enable threads (conversations)\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# This is the in memory store needed to save the memories (i.e. user preferences)\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Construct this relatively simple graph\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(call_model)\n",
    "graph.add_node(update_memory)\n",
    "graph.add_edge(\"update_memory\", END)\n",
    "graph.add_edge(START, \"call_model\")\n",
    "graph.add_conditional_edges(\"call_model\", route, [END, \"update_memory\"])\n",
    "graph = graph.compile(checkpointer=checkpointer, store=in_memory_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d4e33-556d-4fa5-8094-2a076bc21529",
   "metadata": {},
   "source": [
    "## Run graph on one thread\n",
    "\n",
    "We can now run the graph on one thread and give it some information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18bd8679-3a73-4033-bfb4-5093ac1f5d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'call_model': {'messages': [AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 178, 'total_tokens': 188, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_74ba47b4ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-8e7b57b7-8231-4811-945d-a2cf2e1adba3-0', usage_metadata={'input_tokens': 178, 'output_tokens': 10, 'total_tokens': 188})]}}\n",
      "{'call_model': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mpPvip6DbMYkZueShwDDws3y', 'function': {'arguments': '{\"fact\":\"User likes pepperoni pizza\",\"topic\":\"Food\"}', 'name': 'Info'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 200, 'total_tokens': 221, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_74ba47b4ac', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5cd7b512-071b-49ac-9932-de0130774a1e-0', tool_calls=[{'name': 'Info', 'args': {'fact': 'User likes pepperoni pizza', 'topic': 'Food'}, 'id': 'call_mpPvip6DbMYkZueShwDDws3y', 'type': 'tool_call'}], usage_metadata={'input_tokens': 200, 'output_tokens': 21, 'total_tokens': 221})]}}\n",
      "{'update_memory': {'messages': [{'role': 'tool', 'content': 'Saved!', 'tool_call_id': 'call_mpPvip6DbMYkZueShwDDws3y'}]}}\n",
      "{'call_model': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Jqn1cbXFQcVl4hGrbB660kw1', 'function': {'arguments': '{\"fact\":\"User just moved to San Francisco\",\"topic\":\"Location\"}', 'name': 'Info'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 246, 'total_tokens': 268, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_74ba47b4ac', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-9490584d-1678-47db-bb0a-d0797a7af7c3-0', tool_calls=[{'name': 'Info', 'args': {'fact': 'User just moved to San Francisco', 'topic': 'Location'}, 'id': 'call_Jqn1cbXFQcVl4hGrbB660kw1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 246, 'output_tokens': 22, 'total_tokens': 268})]}}\n",
      "{'update_memory': {'messages': [{'role': 'tool', 'content': 'Saved!', 'tool_call_id': 'call_Jqn1cbXFQcVl4hGrbB660kw1'}]}}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# First let's just say hi to the AI\n",
    "for update in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"\n",
    "):\n",
    "    print(update)\n",
    "\n",
    "# Let's continue the conversation (by passing the same config) and tell the AI we like pepperoni pizza\n",
    "for update in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"i like pepperoni pizza\"}]},\n",
    "    config,\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(update)\n",
    "\n",
    "# Let's continue the conversation even further (by passing the same config) and tell the AI we live in SF\n",
    "for update in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"i also just moved to SF\"}]},\n",
    "    config,\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f19db5-ff4a-4666-9be5-ef791c39be0a",
   "metadata": {},
   "source": [
    "We can now inspect our in-memory store and verify that we have in fact saved the memories for the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57335ef9-c6fc-40d8-8dd7-dce8a600d05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fact': 'User likes pepperoni pizza', 'topic': 'Food'}\n",
      "{'fact': 'User just moved to San Francisco', 'topic': 'Location'}\n"
     ]
    }
   ],
   "source": [
    "for memory in in_memory_store.search((\"memories\", \"1\")):\n",
    "    print(memory.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c416fa-086a-491d-a7d3-57091f6413e3",
   "metadata": {},
   "source": [
    "## Run graph on a different thread\n",
    "\n",
    "We can now run the graph on a different thread and see that it remembers facts about the user (specifically that the user likes pepperoni pizza and lives in SF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e240f025-ff8b-4d17-beb7-2420c0575dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'call_model': {'messages': [AIMessage(content=\"Since you like pepperoni pizza, I can recommend a few places in San Francisco where you can enjoy some delicious pizza. Here are some options:\\n\\n1. **Tony's Pizza Napoletana** - Located in North Beach, this restaurant is famous for its award-winning pizzas. You can find a variety of styles, including classic pepperoni.\\n\\n2. **Pizza Orgasmica** - Known for its fun atmosphere and a wide range of pizza options, including pepperoni, this spot is a local favorite.\\n\\n3. **Little Star Pizza** - This place offers deep-dish and thin-crust options. Their pepperoni pizza is highly recommended.\\n\\n4. **Pizzeria Delfina** - A popular spot known for its artisanal pizzas, including a delicious pepperoni option.\\n\\n5. **The Pizza Place on Noriega** - A casual eatery offering classic pizza options in the Sunset District.\\n\\nWould you like more options, or are you interested in a specific neighborhood for dinner?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 198, 'prompt_tokens': 205, 'total_tokens': 403, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_74ba47b4ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-e5d7ff2e-1665-49c4-83dd-ec9b3e693b3c-0', usage_metadata={'input_tokens': 205, 'output_tokens': 198, 'total_tokens': 403})]}}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "for update in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"where and what should i eat for dinner? Can you list some restaurants?\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091995d3",
   "metadata": {},
   "source": [
    "Perfect! The AI recommended restaurants in SF, and included a pizza restaurant at the top of it's list.\n",
    "\n",
    "Notice that the `messages` in this new thread do NOT contain the messages from the previous thread since we didn't store them as shared values across the `user_id`. However, the `info` we saved in the previous thread was saved since we passed in the same `user_id` in this new thread.\n",
    "\n",
    "Let's now run the graph for another user to verify that the preferences of the first user are self contained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9bf2c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'call_model': {'messages': [AIMessage(content=\"To help you better, could you please share your location or the area you're interested in? Additionally, do you have any preferences for cuisine or dietary restrictions?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 192, 'total_tokens': 224, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_74ba47b4ac', 'finish_reason': 'stop', 'logprobs': None}, id='run-fba82c81-703a-4f5e-bd1a-1edf5876425e-0', usage_metadata={'input_tokens': 192, 'output_tokens': 32, 'total_tokens': 224})]}}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"2\"}}\n",
    "\n",
    "for update in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"where and what should i eat for dinner? Can you list some restaurants?\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7086cea",
   "metadata": {},
   "source": [
    "Perfect! The graph has forgotten all of the previous preferences and has to ask the user for it's location and dietary preferences.\n",
    "\n",
    "We can also verify that there are no memories stored for user \"2\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dea8be21-c4a0-434c-87ab-706c138beb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_memory_store.search((\"memories\", \"2\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
