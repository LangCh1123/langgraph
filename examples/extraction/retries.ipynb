{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f054e8c-b931-4ae8-af2c-ebe20497bcda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdc45b5-8d4d-4667-9087-b13cef2e12af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Type, Union\n",
    "\n",
    "import jsonpatch  # type: ignore[import-untyped]\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, AnyMessage, BaseMessage, ToolMessage\n",
    "from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_core.runnables import Runnable, RunnableConfig, RunnableLambda\n",
    "from pydantic import BaseModel\n",
    "from pydantic.v1 import BaseModel as BaseModelV1, Field as FieldV1\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class RetryValues(TypedDict):\n",
    "    messages: List[AnyMessage]\n",
    "    response: BaseMessage\n",
    "\n",
    "\n",
    "class RetryStrategy(TypedDict):\n",
    "    fallback_runnable: Optional[Runnable]\n",
    "    merge_results: Optional[Callable[[RetryValues], Union[dict, list]]]\n",
    "    num_retries: int\n",
    "\n",
    "\n",
    "def with_structured_output(\n",
    "    bound_llm: Runnable,\n",
    "    schema: Type[BaseModel],\n",
    "    retry_strategy: Optional[RetryStrategy] = None,\n",
    "):\n",
    "    name = bound_llm.name or \"runnable_with_retries\"\n",
    "    merge_results = (\n",
    "        retry_strategy[\"merge_results\"] if retry_strategy is not None else None\n",
    "    )\n",
    "    validator = _create_validator(\n",
    "        schema,\n",
    "        merge_results=merge_results,\n",
    "    )\n",
    "    num_retries = retry_strategy[\"num_retries\"] if retry_strategy is not None else 0\n",
    "\n",
    "    def _handle_errors(\n",
    "        messages: List[BaseMessage],\n",
    "        response: Optional[AIMessage],\n",
    "        e: Exception,\n",
    "        include_schema: bool = False,\n",
    "    ) -> list:\n",
    "        results: list = messages.copy()\n",
    "        if response is not None:\n",
    "            results.append(response)\n",
    "            content = f\"Error:\\n\\n```\\n{repr(e)}\\n```\\n\"\n",
    "            if include_schema:\n",
    "                content += (\n",
    "                    \"Expected Parameter Schema:\\n\\n\"\n",
    "                    + f\"```json\\n{schema.model_json_schema()}\\n```\\n\"\n",
    "                )\n",
    "            elif merge_results is not None:\n",
    "                merged = merge_results({\"response\": response, \"messages\": messages})  # type: ignore\n",
    "                content += f\"Current Patched Response:\\n\\n```json\\n{merged}\\n```\\n\"\n",
    "            content += (\n",
    "                \"Recall the function correctly; fix the errors. \"\n",
    "                \"You MUST patch the errors; schema validity trumps all other concerns.\"\n",
    "                \" Use your best guess if needed, for no further user input is forthcoming.\"\n",
    "            )\n",
    "            results.append(\n",
    "                ToolMessage(\n",
    "                    content=content,\n",
    "                    tool_call_id=response.additional_kwargs[\"tool_calls\"][0][\"id\"],\n",
    "                )\n",
    "            )\n",
    "            if len(response.tool_calls) > 1:\n",
    "                for tool_call in response.tool_calls[1:]:\n",
    "                    results.append(\n",
    "                        ToolMessage(\n",
    "                            content=\"Error in one or more tool calls. Please fix all errors.\",\n",
    "                            tool_call_id=str(tool_call[\"id\"]),\n",
    "                        )\n",
    "                    )\n",
    "        else:\n",
    "            results.append(\n",
    "                (\n",
    "                    \"user\",\n",
    "                    f\"Your format was incorrect. Please respond after\"\n",
    "                    f\" fixing the following errors:\\n\\n```\\n{repr(e)}\\n```\",\n",
    "                )\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    def invoke_with_retries(prompt_value: PromptValue, config: RunnableConfig):\n",
    "        messages = prompt_value.to_messages()\n",
    "        response = None\n",
    "        runnable_ = bound_llm\n",
    "        for i in range(num_retries):\n",
    "            include_schema = False\n",
    "            try:\n",
    "                config[\"metadata\"][\"attempt\"] = i\n",
    "                response = runnable_.invoke(messages, config=config)\n",
    "                return validator.invoke(\n",
    "                    {\"response\": response, \"messages\": messages}, config=config\n",
    "                )\n",
    "            except Exception as e:\n",
    "                if retry_strategy is not None and retry_strategy[\"fallback_runnable\"]:\n",
    "                    runnable_ = retry_strategy[\"fallback_runnable\"]\n",
    "                    if i == 0:\n",
    "                        include_schema = True\n",
    "                if i == num_retries - 1:\n",
    "                    raise e\n",
    "                messages = _handle_errors(\n",
    "                    messages, response, e, include_schema=include_schema\n",
    "                )\n",
    "\n",
    "        raise ValueError(\"Should not reach here\")\n",
    "\n",
    "    async def ainvoke_with_retries(prompt_value: PromptValue, config: RunnableConfig):\n",
    "        messages = prompt_value.to_messages()\n",
    "        response = None\n",
    "        runnable_ = bound_llm\n",
    "        for i in range(num_retries):\n",
    "            include_schema = False\n",
    "            try:\n",
    "                config[\"metadata\"][\"attempt\"] = i\n",
    "                response = await runnable_.ainvoke(messages, config=config)\n",
    "                return validator.invoke(\n",
    "                    {\"response\": response, \"messages\": messages}, config=config\n",
    "                )\n",
    "            except Exception as e:\n",
    "                if retry_strategy is not None and retry_strategy[\"fallback_runnable\"]:\n",
    "                    runnable_ = retry_strategy[\"fallback_runnable\"]\n",
    "                    if i == 0:\n",
    "                        include_schema = True\n",
    "                if i == num_retries - 1:\n",
    "                    raise e\n",
    "                messages = _handle_errors(\n",
    "                    messages, response, e, include_schema=include_schema\n",
    "                )\n",
    "        raise ValueError(\"Should not reach here\")\n",
    "\n",
    "    return RunnableLambda(invoke_with_retries, ainvoke_with_retries, name=name)\n",
    "\n",
    "\n",
    "# TODO: This needs to be updated to handle multi-tool calling properly\n",
    "def _create_validator(\n",
    "    schema: Type[BaseModel],\n",
    "    merge_results: Optional[Callable[[RetryValues], Any]] = None,\n",
    ") -> Runnable:\n",
    "    def validate_args(inputs: Union[List, Dict]) -> Union[dict, List[dict]]:\n",
    "        if isinstance(inputs, list):\n",
    "            return [\n",
    "                schema.model_validate(i).model_dump(exclude_none=True) for i in inputs\n",
    "            ]\n",
    "        else:\n",
    "            return schema.model_validate(inputs).model_dump(exclude_none=True)\n",
    "\n",
    "    if merge_results is not None:\n",
    "        validate: Runnable = RunnableLambda(merge_results)\n",
    "    else:\n",
    "        validate = (\n",
    "            (lambda x: x[\"response\"])\n",
    "            | RunnableLambda(_get_args)\n",
    "            | JsonOutputToolsParser()\n",
    "            | _get_args\n",
    "        )\n",
    "    validate = validate | validate_args\n",
    "    return validate.with_config(run_name=\"ValidateStructuredOutput\")\n",
    "\n",
    "\n",
    "class JsonPatch(BaseModelV1):\n",
    "    \"\"\"A JSON Patch document represents an operation to be performed on a JSON document.\n",
    "\n",
    "    Note that the op and path are ALWAYS required. Value is required for ALL operations except 'remove'.\n",
    "    Examples:\n",
    "\n",
    "    ```json\n",
    "    {\"op\": \"add\", \"path\": \"/a/b/c\", \"patch_value\": 1}\n",
    "    {\"op\": \"replace\", \"path\": \"/a/b/c\", \"patch_value\": 2}\n",
    "    {\"op\": \"remove\", \"path\": \"/a/b/c\"}\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    op: Literal[\"add\", \"remove\", \"replace\"] = FieldV1(\n",
    "        ...,\n",
    "        description=\"The operation to be performed. Must be one of 'add', 'remove', 'replace'.\",\n",
    "    )\n",
    "    path: str = FieldV1(\n",
    "        ...,\n",
    "        description=\"A JSON Pointer path that references a location within the target document where the operation is performed.\",\n",
    "    )\n",
    "    value: Any = FieldV1(\n",
    "        ...,\n",
    "        description=\"The value to be used within the operation. REQUIRED for 'add', 'replace', and 'test' operations.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class PatchFunctionParameters(BaseModelV1):\n",
    "    \"\"\"Respond with all JSONPatch operation to correct validation errors caused by passing in incorrect or incomplete parameters in a previous tool call.\"\"\"\n",
    "\n",
    "    reasoning: str = FieldV1(\n",
    "        ...,\n",
    "        description=\"Think step-by-step, listing each validation error and the\"\n",
    "        \" JSONPatch operation needed to correct it. \"\n",
    "        \"Cite the fields in the JSONSchema you referenced in developing this plan.\",\n",
    "    )\n",
    "    patches: List[JsonPatch] = FieldV1(\n",
    "        ...,\n",
    "        description=\"A list of JSONPatch operations to be applied to the previous tool call's response.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_jsonpatch_retry_strategy(\n",
    "    llm: BaseChatModel,\n",
    "    num_retries: int = 3,\n",
    ") -> RetryStrategy:\n",
    "    return {\n",
    "        \"fallback_runnable\": llm.bind_tools(  # type: ignore\n",
    "            [PatchFunctionParameters], tool_choice=\"PatchFunctionParameters\"\n",
    "        ),\n",
    "        \"merge_results\": _merge_results,\n",
    "        \"num_retries\": num_retries,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_default_retry_strategy(\n",
    "    num_retries: int = 0,\n",
    ") -> RetryStrategy:\n",
    "    return {\n",
    "        \"fallback_runnable\": None,\n",
    "        \"merge_results\": None,\n",
    "        \"num_retries\": num_retries,\n",
    "    }\n",
    "\n",
    "\n",
    "def _get_args(x: Union[dict, list]):\n",
    "    if isinstance(x, dict):\n",
    "        return x.get(\"args\") or x\n",
    "    elif isinstance(x, list):\n",
    "        return [y.get(\"args\") or y for y in x]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def _merge_results(results: RetryValues) -> dict:\n",
    "    # Get all the AI messages and apply json patches\n",
    "    previous_convo = results[\"messages\"]\n",
    "    ai_messages: list = [m for m in previous_convo if m.type == \"ai\"] + [\n",
    "        results[\"response\"]\n",
    "    ]\n",
    "    parser = (JsonOutputToolsParser() | _get_args).with_config(run_name=\"ParseOutputs\")\n",
    "    parsed = [tool for tools in parser.batch(ai_messages) for tool in tools]\n",
    "    initial_response = parsed[0]\n",
    "    patches = parsed[1:]\n",
    "    if patches:\n",
    "        operations = [\n",
    "            patch for step_patches in patches for patch in step_patches[\"patches\"]\n",
    "        ]\n",
    "        initial_response = jsonpatch.apply_patch(initial_response, operations)\n",
    "\n",
    "    return initial_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa44f11-7685-4a7b-aa19-f03b9088119c",
   "metadata": {},
   "source": [
    "## Regular Extraction with Retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4826a58c-ac61-45e2-bd62-535a48b55096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "823e78c7-2ebf-4bb1-a119-c9a5c23b7444",
   "metadata": {},
   "source": [
    "#### Graph inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0972321f-794e-4dd2-a5b3-0d9c313456ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "class AlwaysChooseMe(BaseModel):\n",
    "    \"\"\"Invoke to get the answer.\"\"\"\n",
    "\n",
    "    question: str\n",
    "    reason: str = Field(description=\"Reason for asking this question\")\n",
    "\n",
    "    @validator(\"reason\")\n",
    "    def reason_contains_apology(cls, reason: str):\n",
    "        if \"sorry\" not in reason.lower():\n",
    "            raise ValueError(f\"You MUST apologize (sorry) in the reason.\")\n",
    "\n",
    "\n",
    "tools = [AlwaysChooseMe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "baf669a0-04ee-492d-80d8-8fcb658ed128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Any, Literal, Optional, Sequence, Union\n",
    "\n",
    "from langchain_core.messages import AIMessage, AnyMessage, ToolCall, ToolMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, ValidationError\n",
    "from langchain_core.runnables import RunnableConfig, chain as as_runnable\n",
    "from langchain_core.runnables.config import get_executor_for_config\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langgraph.utils import RunnableCallable\n",
    "\n",
    "\n",
    "def _default_format_error(error: BaseException, schema: BaseModel):\n",
    "    return f\"{repr(error)}\\n\\nRespond after fixing all validation errors.\"\n",
    "\n",
    "\n",
    "class ValidationNode(RunnableCallable):\n",
    "    \"\"\"\n",
    "    A node that runs the tools requested in the last AIMessage. It can be used\n",
    "    either in StateGraph with a \"messages\" key or in MessageGraph. If multiple\n",
    "    tool calls are requested, they will be run in parallel. The output will be\n",
    "    a list of ToolMessages, one for each tool call.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tools: Sequence[Union[BaseTool, BaseModel]],\n",
    "        *,\n",
    "        name: str = \"tools\",\n",
    "        format_error: Optional[Callable[[BaseException, BaseModel], str]] = None,\n",
    "        tags: Optional[list[str]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(self._func, self._afunc, name=name, tags=tags, trace=False)\n",
    "        self._format_error = format_error or _default_format_error\n",
    "        self.schemas_by_name = {}\n",
    "        for tool in tools:\n",
    "            if isinstance(tool, BaseTool):\n",
    "                self.schemas_by_name[tool.name] = tool.args_schema\n",
    "            else:\n",
    "                # Pydantic base model\n",
    "                self.schemas_by_name[tool.__name__] = tool\n",
    "\n",
    "    def _func(\n",
    "        self, input: Union[list[AnyMessage], dict[str, Any]], config: RunnableConfig\n",
    "    ) -> Any:\n",
    "        if isinstance(input, list):\n",
    "            output_type = \"list\"\n",
    "            message: AnyMessage = input[-1]\n",
    "        elif messages := input.get(\"messages\", []):\n",
    "            output_type = \"dict\"\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "\n",
    "        if not isinstance(message, AIMessage):\n",
    "            raise ValueError(\"Last message is not an AIMessage\")\n",
    "\n",
    "        @as_runnable\n",
    "        def run_one(call: ToolCall):\n",
    "            schema = self.schemas_by_name[call[\"name\"]]\n",
    "            try:\n",
    "                output = schema.validate(call[\"args\"])\n",
    "                return ToolMessage(\n",
    "                    content=\"Schema is correct\",\n",
    "                    name=call[\"name\"],\n",
    "                    tool_call_id=call[\"id\"],\n",
    "                )\n",
    "            except ValidationError as e:\n",
    "                return ToolMessage(\n",
    "                    content=self._format_error(e, schema),\n",
    "                    name=call[\"name\"],\n",
    "                    tool_call_id=call[\"id\"],\n",
    "                )\n",
    "\n",
    "        with get_executor_for_config(config) as executor:\n",
    "            outputs = [\n",
    "                *executor.map(lambda x: run_one.invoke(x, config), message.tool_calls)\n",
    "            ]\n",
    "            if output_type == \"list\":\n",
    "                return outputs\n",
    "            else:\n",
    "                return {\"messages\": outputs}\n",
    "\n",
    "    async def _afunc(\n",
    "        self, input: Union[list[AnyMessage], dict[str, Any]], config: RunnableConfig\n",
    "    ) -> Any:\n",
    "        if isinstance(input, list):\n",
    "            output_type = \"list\"\n",
    "            message: AnyMessage = input[-1]\n",
    "        elif messages := input.get(\"messages\", []):\n",
    "            output_type = \"dict\"\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "\n",
    "        if not isinstance(message, AIMessage):\n",
    "            raise ValueError(\"Last message is not an AIMessage\")\n",
    "\n",
    "        @as_runnable\n",
    "        async def run_one(call: ToolCall):\n",
    "            schema = self.schemas_by_name[call[\"name\"]]\n",
    "            try:\n",
    "                output = schema.validate(call[\"args\"])\n",
    "                return ToolMessage(\n",
    "                    content=\"Schema is correct\",\n",
    "                    name=call[\"name\"],\n",
    "                    tool_call_id=call[\"id\"],\n",
    "                )\n",
    "            except ValidationError as e:\n",
    "                return ToolMessage(\n",
    "                    content=self._format_error(e, schema),\n",
    "                    name=call[\"name\"],\n",
    "                    tool_call_id=call[\"id\"],\n",
    "                    is_exception=True,\n",
    "                )\n",
    "\n",
    "        outputs = await asyncio.gather(\n",
    "            *(run_one.ainvoke(call, config) for call in message.tool_calls)\n",
    "        )\n",
    "        if output_type == \"list\":\n",
    "            return outputs\n",
    "        else:\n",
    "            return {\"messages\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99798b61-e141-44de-b604-d1394209dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _default_aggregator(messages: list) -> list:\n",
    "    for m in messages[::-1]:\n",
    "        if m.type == \"ai\":\n",
    "            return [m]\n",
    "    return []\n",
    "\n",
    "\n",
    "class Finalizer:\n",
    "    def __init__(self, aggregator: Optional[Callable[list, AIMessage]] = None):\n",
    "        self._aggregator = aggregator or _default_aggregator\n",
    "\n",
    "    def __call__(self, state: State):\n",
    "        \"\"\"Return just the AI message.\"\"\"\n",
    "        return {\n",
    "            \"messages\": {\n",
    "                \"finalize\": self._aggregator(state[\"messages\"]),\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a941ddd6-47e2-4315-af4a-b264ef924024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "def add_or_overwrite_messages(left, right):\n",
    "    if isinstance(right, dict) and \"finalize\" in right:\n",
    "        finalized = right[\"finalize\"]\n",
    "        for m in finalized:\n",
    "            if m.id is None:\n",
    "                m.id = str(uuid.uuid4())\n",
    "        return finalized\n",
    "    return add_messages(left, right)\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_or_overwrite_messages]\n",
    "\n",
    "\n",
    "def create_llm_with_retries(\n",
    "    llm,\n",
    "    tools,\n",
    "    aggregate_messages: Optional[Callable[list, AIMessage]] = None,\n",
    "    format_error: Optional[Callable[[BaseException, BaseModel], str]] = None,\n",
    "    num_attempts: int = 3,\n",
    "):\n",
    "    builder = StateGraph(State)\n",
    "    builder.add_node(\"llm\", lambda x: {\"messages\": [llm.invoke(x[\"messages\"])]})\n",
    "\n",
    "    builder.add_node(\"validator\", ValidationNode(tools, format_error=format_error))\n",
    "    builder.add_edge(\"llm\", \"validator\")\n",
    "\n",
    "    builder.set_entry_point(\"llm\")\n",
    "    attempt_num = 0\n",
    "\n",
    "    def route_validation(state: State) -> Literal[\"llm\", \"finalizer\"]:\n",
    "        nonlocal attempt_num\n",
    "        if attempt_num >= num_attempts:\n",
    "            return \"finalizer\"\n",
    "        attempt_num += 1\n",
    "        for m in state[\"messages\"][::-1]:\n",
    "            if m.type == \"ai\":\n",
    "                break\n",
    "            if getattr(m, \"is_exception\", None):\n",
    "                return \"llm\"\n",
    "        return \"finalizer\"\n",
    "\n",
    "    builder.add_conditional_edges(\"validator\", route_validation)\n",
    "    builder.add_node(\"finalizer\", Finalizer(aggregate_messages))\n",
    "    builder.set_finish_point(\"finalizer\")\n",
    "    return builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "086fb367-6ffd-4312-b172-563ba8acc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_llm = llm.bind_tools(tools, tool_choice=tools[0].__name__)\n",
    "graph = create_llm_with_retries(bound_llm, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "27425a1f-df67-4a0e-afd2-360049dd155c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8sbx0LXMOTE58bNi5j49R75H', 'function': {'arguments': '{\"question\":\"4\",\"reason\":\"I\\'m sorry, the user requested the answer to number 4.\"}', 'name': 'AlwaysChooseMe'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 159, 'total_tokens': 181}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-33ebdf87-766c-492a-b8a9-97ff3643ffee-0', tool_calls=[{'name': 'AlwaysChooseMe', 'args': {'question': '4', 'reason': \"I'm sorry, the user requested the answer to number 4.\"}, 'id': 'call_8sbx0LXMOTE58bNi5j49R75H'}])]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await graph.ainvoke({\"messages\": [(\"user\", \"What's the answer to number 4?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e1962-7f23-463d-b91d-8907c1330369",
   "metadata": {},
   "source": [
    "## JSONPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01891c4-4187-4a75-9eda-644a7c2355f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_llm = llm.bind_tools(tools, tool_choice=tools[0].__name__)\n",
    "graph = create_llm_with_retries(bound_llm, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab55b06b-350a-418c-91c3-432c00dd6ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
