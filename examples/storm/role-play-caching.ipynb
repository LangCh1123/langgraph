{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815dd065-979f-46ed-a465-87fa07b40fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7c2e3-d3ce-4f4b-b63b-b0f1a7d2db09",
   "metadata": {},
   "source": [
    "### LLM\n",
    "\n",
    "Set `ANTHROPIC_API_KEY` and load LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174f81f-e48a-4fea-8519-b62d4fc04e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b782f79f-f8b3-451c-ae64-865324843bb2",
   "metadata": {},
   "source": [
    "Ensure `langchain-anthropic>=0.1.23` for caching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c202ce1a-839e-4125-a6e7-2aa256836e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlm/miniforge3/envs/llama2/lib/python3.11/site-packages/langchain_core/utils/utils.py:234: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", \n",
    "                    extra_headers={\"anthropic-beta\":\"prompt-caching-2024-07-31\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4923b01-391f-4af6-a481-62096b48bb35",
   "metadata": {},
   "source": [
    "### State\n",
    "\n",
    "Let's define state for for graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d516890-288e-4363-a106-6656e4d124cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Optional\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    topic: str\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    analyst: str\n",
    "    editor_feedback: str\n",
    "    interviews: list\n",
    "    reports: list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3930c5c5-7e86-4d69-8c3a-8abc6ff970b7",
   "metadata": {},
   "source": [
    "### Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f20aa751-6acb-49af-af4c-21f3236581d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.runnables import chain as as_runnable\n",
    "\n",
    "gen_qn_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \n",
    "            \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
    "\n",
    "            Your goal is boil down to interesting and specific insights related to your topic.\n",
    "\n",
    "            1. Interesting: Insights that people will find surprising or non-obvious.\n",
    "            \n",
    "            2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
    "    \n",
    "            Here is your topic of focus and set of goals: {persona}\n",
    "            \n",
    "            Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
    "\n",
    "            Continue to ask questions to drill down and refine your understanding of the topic.\n",
    "            \n",
    "            When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
    "\n",
    "            Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\",\n",
    "        \n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "@as_runnable\n",
    "def generate_question(state: InterviewState):\n",
    "    \"\"\" Node to generate a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Generate question \n",
    "    gen_question_chain = gen_qn_prompt.partial(persona=analyst) | llm   \n",
    "    result = gen_question_chain.invoke({\"messages\": messages})\n",
    "    \n",
    "    # Write messages to state\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90cf0510-7385-45e4-a692-2e5d9c62f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# Topic\n",
    "topic = \"Technical innovations related to the Meta LLlama3.1 LLM\"\n",
    "\n",
    "# Starter message\n",
    "messages = [HumanMessage(f\"So you said you were researching {topic}?\")]\n",
    "\n",
    "# Analyst role\n",
    "role = \"My focus is on the training infrasturcture -- such as # GPU, networking, etc -- used for llama3.1.\"\n",
    "\n",
    "# Analyst to test \n",
    "interview_state = {\"analyst\": role, \"messages\": messages}\n",
    "\n",
    "# Generate\n",
    "question = generate_question.invoke(interview_state)\n",
    "\n",
    "# Append to messages\n",
    "messages.extend(question['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3ef18d3-52a7-46aa-a308-cb889186a837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "So you said you were researching Technical innovations related to the Meta LLlama3.1 LLM?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello there! I'm Dr. Emily Chen, an AI infrastructure analyst specializing in large language model architectures. Yes, that's correct - I'm particularly interested in the training infrastructure used for Meta's Llama 3.1 model. \n",
      "\n",
      "To start off, could you tell me about any specific details you know regarding the GPU setup used for training Llama 3.1? I'm especially curious about the number and type of GPUs that might have been employed.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c76bf6-000d-4a31-b971-ff8ab9883544",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "\n",
    "We have an expert with access to the [full llama3.1 paper](https://scontent-sjc3-1.xx.fbcdn.net/v/t39.2365-6/453304228_1160109801904614_7143520450792086005_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=LQo4g8kQLc4Q7kNvgH9HnsN&_nc_ht=scontent-sjc3-1.xx&oh=00_AYD-ZjbsTOnF3JKDvb8di6uXkqNlTg5u-8ZVvfG8sT8flg&oe=66C2DA07).\n",
    "\n",
    "Download and supply local path.\n",
    "\n",
    "Now, `formatted_pages` will be our fixed context that we want to [cache](https://www.anthropic.com/news/prompt-caching)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f317a74-aafe-4c01-9afc-10c9701f2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load llama3.1 paper (pdf)\n",
    "loader = PyPDFLoader(\"/Users/rlm/Desktop/llama_3_1_paper.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Full paper, except for references \n",
    "all_pages_except_references=pages[:100]\n",
    "\n",
    "# Format\n",
    "formatted_pages = \"\\n --- \\n\".join(\n",
    "    [\n",
    "        f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "        for doc in all_pages_except_references\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69c93de4-86e8-4225-848c-3c29a6edc10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For RAG with context\n",
    "system_message = {\"role\":\"system\",\n",
    "            \"content\":[{\n",
    "                \"type\":\"text\",\n",
    "                \"text\":f\"To answer question in the below conversation, use this context: {formatted_pages}\",\n",
    "                \"cache_control\":{\"type\":\"ephemeral\"}\n",
    "            }]} \n",
    "\n",
    "@as_runnable\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\" Node to answer a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Add context\n",
    "    messages.insert(0, system_message)\n",
    "   \n",
    "    # Answer question\n",
    "    answer = llm.invoke(messages)  \n",
    "    \n",
    "    # Name the message as coming from the expert\n",
    "    answer.name = \"expert\"\n",
    "    \n",
    "    # Append it to state\n",
    "    return {\"messages\": [answer]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a057141-3749-438c-8764-fdf9c2972569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "interview_state = {\"messages\": messages}\n",
    "answer = generate_answer.invoke(interview_state)\n",
    "messages.extend(answer['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d110b07d-e066-413e-b385-f4fba0da6de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "So you said you were researching Technical innovations related to the Meta LLlama3.1 LLM?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello there! I'm Dr. Emily Chen, an AI infrastructure analyst specializing in large language model architectures. Yes, that's correct - I'm particularly interested in the training infrastructure used for Meta's Llama 3.1 model. \n",
      "\n",
      "To start off, could you tell me about any specific details you know regarding the GPU setup used for training Llama 3.1? I'm especially curious about the number and type of GPUs that might have been employed.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: expert\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46626e3-8dc4-4837-ac1f-39d172ab3731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7dd8ff-4346-45eb-bda5-ac34cb5c619a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
