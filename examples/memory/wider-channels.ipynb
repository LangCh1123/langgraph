{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7240d5b5-9dac-4070-8a9e-2350fb01e0be",
   "metadata": {},
   "source": [
    "# How to share state between threads\n",
    "\n",
    "By default, state in a graph is scoped to that thread.\n",
    "LangGraph also allows you to specify a \"scope\" for a given key/value pair that exists between threads.\n",
    "\n",
    "In this notebook we will go through an example of how to construct and use such a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c550b5-1954-496b-8b9d-800361af17dc",
   "metadata": {},
   "source": [
    "## Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f303d6-612e-4e34-bf36-29d4ed25d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.graph import START, END\n",
    "from langgraph.graph.message import MessageGraph, add_messages, MessagesState\n",
    "from langgraph.graph.state import StateGraph\n",
    "from langgraph.store.memory import MemoryStore\n",
    "from langgraph.managed.shared_value import SharedValue\n",
    "from typing import TypedDict, Annotated, Any\n",
    "import uuid\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    # We use an info key to track information\n",
    "    # This is scoped to a user_id, so it will be information specific to each user\n",
    "    info: Annotated[dict, SharedValue.on(\"user_id\")]\n",
    "\n",
    "\n",
    "# We will give this as a tool to the agent\n",
    "# This will let the agent call this tool to save a fact\n",
    "class Info(TypedDict):\n",
    "    fact: str\n",
    "\n",
    "\n",
    "# This is the prompt we give the agent\n",
    "# We will pass known info into the prompt\n",
    "# We will tell it to use the Info tool to save more\n",
    "prompt = \"\"\"You are helpful assistant.\n",
    "\n",
    "Here is what you know about the user:\n",
    "\n",
    "<info>\n",
    "{info}\n",
    "</info>\n",
    "\n",
    "Help out the user. If the user tells you things that you may want to remember for the future, save them using the `Info` tool.\"\"\"\n",
    "\n",
    "\n",
    "# We give the model access to the Info tool\n",
    "model = ChatOpenAI().bind_tools([Info])\n",
    "\n",
    "\n",
    "# Our first node - this will call the model\n",
    "def call_model(state):\n",
    "    # We get all facts and assemble them into a string\n",
    "    facts = [d['fact'] for d in state['info'].values()]\n",
    "    info = \"\\n\".join(facts)\n",
    "    # Format system prompt\n",
    "    system_msg = prompt.format(info=info)\n",
    "    # Call model\n",
    "    response = model.invoke([{\"role\": \"system\", \"content\": system_msg}] + state['messages'])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Routing function to decide what to do next\n",
    "# If no tool calls, then we end\n",
    "# If tool calls, then we update memory\n",
    "def route(state):\n",
    "    if len(state['messages'][-1].tool_calls) == 0:\n",
    "        return END\n",
    "    else:\n",
    "        return \"update_memory\"\n",
    "\n",
    "\n",
    "# This function is responsible for updating the memory\n",
    "def update_memory(state):\n",
    "    tool_calls = []\n",
    "    memories = {}\n",
    "    # Each tool call is a new memory to save\n",
    "    for tc in state['messages'][-1].tool_calls:\n",
    "        # We append ToolMessages (to pass back to the LLM)\n",
    "        # This is needed because OpenAI requires each tool call be followed by a ToolMessage\n",
    "        tool_calls.append({\"role\": \"tool\", \"content\": \"Saved!\", \"tool_call_id\": tc['id']})\n",
    "        # We create a new memory from this tool call\n",
    "        memories[str(uuid.uuid4())] = {\"fact\": tc['args']['fact']}\n",
    "    # Return the messages and memories to update the state with\n",
    "    return {\"messages\": tool_calls, \"info\": memories}\n",
    "\n",
    "\n",
    "# This is the in memory checkpointer we will use\n",
    "# We need this because we want to enable threads (conversations)\n",
    "memory = MemorySaver()\n",
    "\n",
    "# This is the in memory Key Value store\n",
    "# This is needed to save the memories\n",
    "kv = MemoryStore()\n",
    "\n",
    "# Construct this relatively simple graph\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(call_model)\n",
    "graph.add_node(update_memory)\n",
    "graph.add_edge(\"update_memory\", END)\n",
    "graph.add_edge(START, \"call_model\")\n",
    "graph.add_conditional_edges(\"call_model\", route)\n",
    "graph = graph.compile(checkpointer=memory, store=kv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d4e33-556d-4fa5-8094-2a076bc21529",
   "metadata": {},
   "source": [
    "## Run graph on one thread\n",
    "\n",
    "We can now run the graph on one thread and give it some information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18bd8679-3a73-4033-bfb4-5093ac1f5d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'call_model': {'messages': [AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 185, 'total_tokens': 195}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-65eca425-e789-433f-ba92-859dc77835f6-0', usage_metadata={'input_tokens': 185, 'output_tokens': 10, 'total_tokens': 195})]}}\n",
      "{'call_model': {'messages': [AIMessage(content='Pizza is a popular choice! What is your favorite type of pizza topping?', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 205, 'total_tokens': 221}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7204558e-6864-4d71-b0e1-6b0389afaa66-0', usage_metadata={'input_tokens': 205, 'output_tokens': 16, 'total_tokens': 221})]}}\n",
      "{'call_model': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_aHqramLif4ewJfEtwQQC1Ypo', 'function': {'arguments': '{\"fact\":\"Favorite pizza topping: Pepperoni\"}', 'name': 'Info'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 232, 'total_tokens': 250}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-81ebf2a4-f400-4b39-9f96-45c2c5dc16ae-0', tool_calls=[{'name': 'Info', 'args': {'fact': 'Favorite pizza topping: Pepperoni'}, 'id': 'call_aHqramLif4ewJfEtwQQC1Ypo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 232, 'output_tokens': 18, 'total_tokens': 250})]}}\n",
      "{'update_memory': {'messages': [{'role': 'tool', 'content': 'Saved!', 'tool_call_id': 'call_aHqramLif4ewJfEtwQQC1Ypo'}]}}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "for update in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"):\n",
    "    print(update)\n",
    "\n",
    "for update in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"i like pizza\"}]}, config, stream_mode=\"updates\"):\n",
    "    print(update)\n",
    "\n",
    "for update in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"peperroni\"}]}, config, stream_mode=\"updates\"):\n",
    "    print(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c416fa-086a-491d-a7d3-57091f6413e3",
   "metadata": {},
   "source": [
    "## Run graph on a different thread\n",
    "\n",
    "We can now run the graph on a different thread and see that it remembers facts about the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e240f025-ff8b-4d17-beb7-2420c0575dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'call_model': {'messages': [AIMessage(content=\"How about having a delicious pepperoni pizza for dinner? It's a classic choice!\", response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 198, 'total_tokens': 216}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-090270eb-5732-4c02-a137-0fff3b777811-0', usage_metadata={'input_tokens': 198, 'output_tokens': 18, 'total_tokens': 216})]}}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "for update in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"what should i have for dinner?\"}]}, config, stream_mode=\"updates\"):\n",
    "    print(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542478a-603d-4909-a17e-1fd34cbdc50d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
