{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7240d5b5-9dac-4070-8a9e-2350fb01e0be",
   "metadata": {},
   "source": [
    "# How to share state between threads\n",
    "\n",
    "By default, state in a graph is scoped to that thread.\n",
    "LangGraph also allows you to specify a \"scope\" for a given key/value pair that exists between threads. This can be useful for storing information that is shared between threads. For instance, you may want to store information about a user's preferences expressed in one thread, and then use that information in another thread.\n",
    "\n",
    "In this notebook we will go through an example of how to construct and use such a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c550b5-1954-496b-8b9d-800361af17dc",
   "metadata": {},
   "source": [
    "## Create graph\n",
    "\n",
    "In this exampe we will create a graph that will let us store information about a user's preferences. We will do so by defining a state key that will be scoped to a user_id, and allowing the model to populate this field as it deems fit (by providing the model with a tool to save information about the user).\n",
    "\n",
    "Note that shared state keys MUST be of type dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f303d6-612e-4e34-bf36-29d4ed25d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.graph import START, END\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langgraph.graph.state import StateGraph\n",
    "from langgraph.store.memory import MemoryStore\n",
    "from langgraph.managed.shared_value import SharedValue\n",
    "from typing import TypedDict, Annotated, Any\n",
    "import uuid\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    # We use an info key to track information\n",
    "    # This is scoped to a user_id, so it will be information specific to each user\n",
    "    info: Annotated[dict, SharedValue.on(\"user_id\")]\n",
    "\n",
    "\n",
    "# We will give this as a tool to the agent\n",
    "# This will let the agent call this tool to save a fact\n",
    "class Info(TypedDict):\n",
    "    \"\"\"This tool should be called when you want to save a new fact about the user.\n",
    "    \n",
    "    Attributes:\n",
    "        fact (str): A fact about the user.\n",
    "        topic (str): The topic related the fact is about, i.e. Food, Movies, etc.\n",
    "    \"\"\"\n",
    "    fact: str\n",
    "    topic: str\n",
    "\n",
    "\n",
    "# This is the prompt we give the agent\n",
    "# We will pass known info into the prompt\n",
    "# We will tell it to use the Info tool to save more\n",
    "prompt = \"\"\"You are helpful assistant.\n",
    "\n",
    "Here is what you know about the user:\n",
    "\n",
    "<info>\n",
    "{info}\n",
    "</info>\n",
    "\n",
    "Help out the user. If the user tells you things that you may want to remember for the future, save them using the `Info` tool.\"\"\"\n",
    "\n",
    "\n",
    "# We give the model access to the Info tool\n",
    "model = ChatOpenAI().bind_tools([Info])\n",
    "\n",
    "\n",
    "# Our first node - this will call the model\n",
    "def call_model(state):\n",
    "    # We get all facts and assemble them into a string\n",
    "    facts = [d['fact'] for d in state['info'].values()]\n",
    "    info = \"\\n\".join(facts)\n",
    "    # Format system prompt\n",
    "    system_msg = prompt.format(info=info)\n",
    "    # Call model\n",
    "    response = model.invoke([{\"role\": \"system\", \"content\": system_msg}] + state['messages'])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Routing function to decide what to do next\n",
    "# If no tool calls, then we end\n",
    "# If tool calls, then we update memory\n",
    "def route(state):\n",
    "    if len(state['messages'][-1].tool_calls) == 0:\n",
    "        return END\n",
    "    else:\n",
    "        return \"update_memory\"\n",
    "\n",
    "\n",
    "# This function is responsible for updating the memory\n",
    "def update_memory(state):\n",
    "    tool_calls = []\n",
    "    memories = {}\n",
    "    # Each tool call is a new memory to save\n",
    "    for tc in state['messages'][-1].tool_calls:\n",
    "        # We append ToolMessages (to pass back to the LLM)\n",
    "        # This is needed because OpenAI requires each tool call be followed by a ToolMessage\n",
    "        tool_calls.append({\"role\": \"tool\", \"content\": \"Saved!\", \"tool_call_id\": tc['id']})\n",
    "        # We create a new memory from this tool call\n",
    "        memories[str(uuid.uuid4())] = {\"fact\": tc['args']['fact'], \"topic\": tc['args']['topic']}\n",
    "    # Return the messages and memories to update the state with\n",
    "    return {\"messages\": tool_calls, \"info\": memories}\n",
    "\n",
    "\n",
    "# This is the in memory checkpointer we will use\n",
    "# We need this because we want to enable threads (conversations)\n",
    "memory = MemorySaver()\n",
    "\n",
    "# This is the in memory Key Value store\n",
    "# This is needed to save the memories\n",
    "kv = MemoryStore()\n",
    "\n",
    "# Construct this relatively simple graph\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(call_model)\n",
    "graph.add_node(update_memory)\n",
    "graph.add_edge(\"update_memory\", END)\n",
    "graph.add_edge(START, \"call_model\")\n",
    "graph.add_conditional_edges(\"call_model\", route)\n",
    "graph = graph.compile(checkpointer=memory, store=kv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d4e33-556d-4fa5-8094-2a076bc21529",
   "metadata": {},
   "source": [
    "## Run graph on one thread\n",
    "\n",
    "We can now run the graph on one thread and give it some information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18bd8679-3a73-4033-bfb4-5093ac1f5d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'call_model': {'messages': [AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 144, 'total_tokens': 154}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9eb94b18-7159-4dcd-821b-9a63d43627fa-0', usage_metadata={'input_tokens': 144, 'output_tokens': 10, 'total_tokens': 154})]}}\n",
      "{'call_model': {'messages': [AIMessage(content=\"That's great to hear! Pizza is a popular choice for many people. Do you have a favorite type of pizza or toppings that you enjoy?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 164, 'total_tokens': 194}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e4258d54-ccb9-4f40-8174-c1b2c027f2f9-0', usage_metadata={'input_tokens': 164, 'output_tokens': 30, 'total_tokens': 194})]}}\n",
      "{'call_model': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_7D7sHTSQ5jdYJ2Q32qG0uCq4', 'function': {'arguments': '{\"fact\":\"The user likes pepperoni pizza.\",\"topic\":\"Food\"}', 'name': 'Info'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 204, 'total_tokens': 226}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7e261866-8470-408c-8124-c991a94888ad-0', tool_calls=[{'name': 'Info', 'args': {'fact': 'The user likes pepperoni pizza.', 'topic': 'Food'}, 'id': 'call_7D7sHTSQ5jdYJ2Q32qG0uCq4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 204, 'output_tokens': 22, 'total_tokens': 226})]}}\n",
      "{'update_memory': {'messages': [{'role': 'tool', 'content': 'Saved!', 'tool_call_id': 'call_7D7sHTSQ5jdYJ2Q32qG0uCq4'}]}}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "for update in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"):\n",
    "    print(update)\n",
    "\n",
    "for update in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"i like pizza\"}]}, config, stream_mode=\"updates\"):\n",
    "    print(update)\n",
    "\n",
    "for update in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"pepperoni\"}]}, config, stream_mode=\"updates\"):\n",
    "    print(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c416fa-086a-491d-a7d3-57091f6413e3",
   "metadata": {},
   "source": [
    "## Run graph on a different thread\n",
    "\n",
    "We can now run the graph on a different thread and see that it remembers facts about the user (specifically that the user likes pepperoni pizza):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e240f025-ff8b-4d17-beb7-2420c0575dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'call_model': {'messages': [AIMessage(content='How about having some delicious pepperoni pizza for dinner?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 157, 'total_tokens': 169}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d131ac80-ce42-43c8-a084-9cbfd4157017-0', usage_metadata={'input_tokens': 157, 'output_tokens': 12, 'total_tokens': 169})]}}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "for update in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"what should i have for dinner?\"}]}, config, stream_mode=\"updates\"):\n",
    "    print(update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
